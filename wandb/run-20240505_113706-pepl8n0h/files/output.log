













 22%|████████████████████▋                                                                           | 497/2307 [00:28<01:39, 18.14it/s]
 22%|████████████████████▊                                                                           | 500/2307 [00:28<01:39, 18.17it/s][INFO|trainer.py:2881] 2024-05-05 11:37:45,250 >> Saving model checkpoint to ckpts/checkpoint-500
[INFO|configuration_utils.py:461] 2024-05-05 11:37:45,251 >> Configuration saved in ckpts/checkpoint-500/config.json
[INFO|modeling_utils.py:2193] 2024-05-05 11:37:47,537 >> Model weights saved in ckpts/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-05-05 11:37:47,540 >> tokenizer config file saved in ckpts/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-05-05 11:37:47,540 >> Special tokens file saved in ckpts/checkpoint-500/special_tokens_map.json













 43%|█████████████████████████████████████████▏                                                      | 989/2307 [01:02<01:12, 18.16it/s]
 43%|█████████████████████████████████████████▏                                                     | 1000/2307 [01:03<01:12, 18.14it/s][INFO|trainer.py:2881] 2024-05-05 11:38:19,906 >> Saving model checkpoint to ckpts/checkpoint-1000
[INFO|configuration_utils.py:461] 2024-05-05 11:38:19,907 >> Configuration saved in ckpts/checkpoint-1000/config.json
[INFO|modeling_utils.py:2193] 2024-05-05 11:38:22,198 >> Model weights saved in ckpts/checkpoint-1000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-05-05 11:38:22,199 >> tokenizer config file saved in ckpts/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-05-05 11:38:22,199 >> Special tokens file saved in ckpts/checkpoint-1000/special_tokens_map.json













 64%|█████████████████████████████████████████████████████████████                                  | 1483/2307 [01:36<00:45, 18.14it/s]
 65%|█████████████████████████████████████████████████████████████▊                                 | 1500/2307 [01:37<00:44, 18.16it/s][INFO|trainer.py:2881] 2024-05-05 11:38:54,339 >> Saving model checkpoint to ckpts/checkpoint-1500
[INFO|configuration_utils.py:461] 2024-05-05 11:38:54,340 >> Configuration saved in ckpts/checkpoint-1500/config.json
[INFO|modeling_utils.py:2193] 2024-05-05 11:38:54,872 >> Model weights saved in ckpts/checkpoint-1500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-05-05 11:38:54,874 >> tokenizer config file saved in ckpts/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-05-05 11:38:54,874 >> Special tokens file saved in ckpts/checkpoint-1500/special_tokens_map.json












 87%|██████████████████████████████████████████████████████████████████████████████████▎            | 2000/2307 [02:07<00:16, 18.11it/s][INFO|trainer.py:2881] 2024-05-05 11:39:23,583 >> Saving model checkpoint to ckpts/checkpoint-2000
[INFO|configuration_utils.py:461] 2024-05-05 11:39:23,584 >> Configuration saved in ckpts/checkpoint-2000/config.json
{'loss': 0.294, 'learning_rate': 2.6614651061985264e-06, 'epoch': 0.87}
[INFO|modeling_utils.py:2193] 2024-05-05 11:39:24,119 >> Model weights saved in ckpts/checkpoint-2000/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-05-05 11:39:24,120 >> tokenizer config file saved in ckpts/checkpoint-2000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-05-05 11:39:24,120 >> Special tokens file saved in ckpts/checkpoint-2000/special_tokens_map.json








100%|██████████████████████████████████████████████████████████████████████████████████████████████▋| 2299/2307 [02:25<00:00, 18.15it/s]
{'train_runtime': 157.6908, 'train_samples_per_second': 116.995, 'train_steps_per_second': 14.63, 'train_loss': 0.3197085608328169, 'epoch': 1.0}
***** train metrics *****
  epoch                    =        1.0
  train_loss               =     0.3197
  train_runtime            = 0:02:37.69
  train_samples            =      18449
  train_samples_per_second =    116.995
  train_steps_per_second   =      14.63
100%|██████████████████████████████████████████████████████████████████████████████████████████████▉| 2305/2307 [02:25<00:00, 18.14it/s][INFO|trainer.py:1955] 2024-05-05 11:39:42,135 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2307/2307 [02:25<00:00, 15.84it/s]
[INFO|trainer.py:2881] 2024-05-05 11:39:42,140 >> Saving model checkpoint to ckpts/
[INFO|configuration_utils.py:461] 2024-05-05 11:39:42,141 >> Configuration saved in ckpts/config.json
[INFO|modeling_utils.py:2193] 2024-05-05 11:39:42,670 >> Model weights saved in ckpts/pytorch_model.bin
[INFO|tokenization_utils_base.py:2428] 2024-05-05 11:39:42,671 >> tokenizer config file saved in ckpts/tokenizer_config.json
[INFO|tokenization_utils_base.py:2437] 2024-05-05 11:39:42,672 >> Special tokens file saved in ckpts/special_tokens_map.json
[INFO|trainer.py:738] 2024-05-05 11:39:42,710 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: tweet, count, Unnamed: 0, offensive_language, neither, sentence, hate_speech. If tweet, count, Unnamed: 0, offensive_language, neither, sentence, hate_speech are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:3158] 2024-05-05 11:39:42,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3160] 2024-05-05 11:39:42,713 >>   Num examples = 2050
[INFO|trainer.py:3163] 2024-05-05 11:39:42,713 >>   Batch size = 8


100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 257/257 [00:04<00:00, 60.69it/s]
***** eval metrics *****
  epoch                   =        1.0
  eval_accuracy           =     0.9029
  eval_loss               =     0.3257
  eval_runtime            = 0:00:04.25
  eval_samples            =       2050
  eval_samples_per_second =    481.819
  eval_steps_per_second   =     60.404
[INFO|modelcard.py:452] 2024-05-05 11:39:48,018 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9029268292682927}]}